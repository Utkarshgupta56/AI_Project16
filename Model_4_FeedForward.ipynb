{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLXHzkVRg7Tp",
        "outputId": "d48720f0-710e-40b0-8f61-bd9b328229c4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feed Forward"
      ],
      "metadata": {
        "id": "cXboVBcAjzg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "As the model does not have much time to interpret the relation between them so we add a ffd nn so that they can\n",
        "extract the knowledge from each other using Relu activation function.\n",
        "'''\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net=nn.Sequential(\n",
        "        nn.Linear(n_embd,n_embd),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.net(x)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NNQlTohRezA7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from rouge import Rouge\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences\n",
        "block_size = 8 # what is the maximum context\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "n_head = 4\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "with open('/content/input (1).txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        perplexities = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss, perplexity = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "            perplexities[k] = perplexity.item() if perplexity is not None else float('nan')\n",
        "        out[f'{split}_loss'] = losses.mean()\n",
        "        out[f'{split}_perplexity'] = perplexities.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self,head_size):\n",
        "      super().__init__()\n",
        "      self.key=nn.Linear(n_embd,head_size,bias=False) ##Corresponds to the embedding layer, (see BTC shape here C is the embedding vector)\n",
        "      self.query=nn.Linear(n_embd,head_size,bias=False)\n",
        "      self.value=nn.Linear(n_embd,head_size,bias=False)\n",
        "      #Here tril is not the part of nn module to we add tril as buffer, meaning our custom function\n",
        "      self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size))) ##we made an upper triangle matrix above of shape T,T (8,8)\n",
        "\n",
        "#Forward process for the computation here\n",
        "    def forward(self,x):\n",
        "      B,T,C=x.shape\n",
        "      k=self.key(x)\n",
        "      q=self.query(x)\n",
        "      weight=q @ k.transpose(-2,-1) * C**-0.5\n",
        "      weight=weight.masked_fill(self.tril[:T,:T]==0,float('-inf'))\n",
        "      weight=F.softmax(weight,dim=-1)\n",
        "      v=self.value(x)\n",
        "      output=weight@v\n",
        "      return output\n",
        "\n",
        "\n",
        "'''We need to find multiple relation between the words herer , so we use multi-head-attention.\n",
        " We give 4 attention as our output to capture different relation between the words and each attention giving out 8 block of words and concenate\n",
        "them to give an output of 32 which will be equal to our n_embeddings'''\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,num_head,head_size):\n",
        "      super().__init__()\n",
        "      self.head=nn.ModuleList([Head(head_size) for _ in range(num_head)])\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "      output=torch.cat([h(x) for h in self.head],dim=-1) # AS THE CONCAT TO THE CHANNEL SIDE SO DIM=-1(C)\n",
        "      return output\n",
        "\n",
        "'''\n",
        "As the model does not have much time to interpret the relation between them so we add a ffd nn so that they can\n",
        "extract the knowledge from each other using Relu activation function.\n",
        "'''\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net=nn.Sequential(\n",
        "        nn.Linear(n_embd,n_embd),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.net(x)\n",
        "\n",
        "\n",
        "#simple bigram model-4\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.con_head=MultiHeadAttention(4,n_embd//4) # 4 Heads concat to 32 (=n_embd vector)\n",
        "        self.ffdnn=FeedForward(n_embd) ##Add ffd for computation\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb =self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb =self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x =tok_emb + pos_emb # (B,T,C)\n",
        "        x =self.con_head(x) # Implementing the self_attention\n",
        "        x=self.ffdnn(x) ##Calling the ffdnn\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss=None\n",
        "            perplexity=None\n",
        "        else:\n",
        "            B,T,C =logits.shape\n",
        "            logits=logits.view(B*T, C)\n",
        "            targets=targets.view(B*T)\n",
        "            loss=F.cross_entropy(logits, targets)\n",
        "            probabilities=F.softmax(logits, dim=-1)\n",
        "            log_probabilities=torch.log(probabilities)\n",
        "            log_prob=torch.gather(log_probabilities,1,targets.unsqueeze(1)).squeeze(1)\n",
        "            N=targets.size(0)\n",
        "            perplexity=torch.exp(-torch.sum(log_prob)/N)\n",
        "\n",
        "        return logits, loss,perplexity\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond =idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss,perplexity = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits =logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs =F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next =torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx =torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train_loss']:.4f}, val loss {losses['val_loss']:.4f}, PERPLEX_TRAIN {losses['train_perplexity']:.4f} \"+\n",
        "              f\"PERPLEX_VAL {losses['val_perplexity']:.4f}\"\n",
        "        )\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss,perplexity = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n",
        "\n",
        "print(\"#########-----------------------------------------------------------------------------------------------------------------------------###### \\n \\n\")\n",
        "rouge = Rouge()\n",
        "generated_summaries = []\n",
        "context = torch.tensor([[4,24,26]], dtype=torch.long, device=device)\n",
        "summary= decode(m.generate(context, max_new_tokens=2000)[0].tolist())\n",
        "generated_summaries.append(summary)\n",
        "reference_summaries = []\n",
        "reference_summaries.append(text[:2000])\n",
        "print(\"REFERENCE SUMMARY: \\n\"+str(reference_summaries[0][:50])+\"\\n\")\n",
        "print(\"GENERATED SUMMARY: \\n\"+str(generated_summaries[0][:50])+\"\\n\")\n",
        "print(\"ROUGE_SCORES: \\n\")\n",
        "rouge_scores = rouge.get_scores(generated_summaries, reference_summaries)\n",
        "for score in rouge_scores:\n",
        "    print(score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWYtgNQIgk7p",
        "outputId": "0315d7b4-e66f-4685-ff20-bc133149be9c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.008609 M parameters\n",
            "step 0: train loss 4.1996, val loss 4.1995, PERPLEX_TRAIN 66.6638 PERPLEX_VAL 66.6550\n",
            "step 500: train loss 2.5993, val loss 2.6077, PERPLEX_TRAIN 13.5069 PERPLEX_VAL 13.6153\n",
            "step 1000: train loss 2.4629, val loss 2.4651, PERPLEX_TRAIN 11.7818 PERPLEX_VAL 11.8053\n",
            "step 1500: train loss 2.3974, val loss 2.3951, PERPLEX_TRAIN 11.0290 PERPLEX_VAL 11.0156\n",
            "step 2000: train loss 2.3297, val loss 2.3470, PERPLEX_TRAIN 10.3102 PERPLEX_VAL 10.4923\n",
            "step 2500: train loss 2.3018, val loss 2.3221, PERPLEX_TRAIN 10.0318 PERPLEX_VAL 10.2321\n",
            "step 3000: train loss 2.2828, val loss 2.2936, PERPLEX_TRAIN 9.8407 PERPLEX_VAL 9.9490\n",
            "step 3500: train loss 2.2495, val loss 2.2721, PERPLEX_TRAIN 9.5130 PERPLEX_VAL 9.7456\n",
            "step 4000: train loss 2.2435, val loss 2.2468, PERPLEX_TRAIN 9.4644 PERPLEX_VAL 9.4892\n",
            "step 4500: train loss 2.2286, val loss 2.2411, PERPLEX_TRAIN 9.3217 PERPLEX_VAL 9.4320\n",
            "step 4999: train loss 2.2174, val loss 2.2475, PERPLEX_TRAIN 9.2159 PERPLEX_VAL 9.5039\n",
            "\n",
            "Acled! lepin,--\n",
            "Thri, que bave?\n",
            "\n",
            "DOWHENCHESOUNG:\n",
            "I nove, bar\n",
            "th aly plost yevest rail thep iciecl hat ine aing sor he meet! wome? withe beoteg noth and and woens eyothast fors;\n",
            "Hard, to, to thre's the cey, frand whim fbytaet' dlicee ad it cantult to bat ae, alend con are: tith pli!\n",
            "\n",
            "Lornd third sean tou lonoctrett ccaby ancaming pomes:\n",
            "If meed youh not Plorlling tre.\n",
            "\n",
            "CIRYY teiss thins to ime; ther gell robbop:\n",
            "Wakor. Vain, falve.\n",
            "\n",
            "Bus aund is Heith, hims.\n",
            "\n",
            "QUOLERINUS:\n",
            "I he\n",
            "he standed,\n",
            "Rorn'dit barcink thistrnet uldyaimlh of now the, ant aldy karoy ain bill\n",
            "th\n",
            "Bloll,\n",
            "Iuts durs bred, havuink kndelis likn ofrifmed an nos,\n",
            "Thovef you\n",
            "OCARI dearmannoing uare mank,\n",
            "Hamaids ane, the you thand nos hu ald tulll the facits, you inl thark alcir Ros ard, ond so thou crand dom thathre hes we I now pre hou veed ther this wo.\n",
            "Yogh seellly,\n",
            "Bas, ist; thite, to dath cade for\n",
            "I coth!\n",
            "I RE ICHARFORD all Roou?\n",
            "\n",
            "\n",
            "HY tut sentive at oun wethato tatuns for to forstetang coo.\n",
            "\n",
            "We iet, hat foillll with: sone pend dis-yets pay yre ourd; aner hord, hith! I throw: shome adiuch couthe at as he aruseem, gack eoors tar disin:\n",
            "My to uniall, bel of youd sataurdest and thit of swerem dint him ply mit sees;\n",
            "At--ret bred the suth the youngle's rin\n",
            "eve Ra kroy int wherpare not her hordin clowt.\n",
            "\n",
            "MONGS:\n",
            "Tot when den ble wakith her the houn Plethe indoth, liendy's and tith, may tamy's pund:\n",
            "'Y Dotch pop lie tan tithrairg, petewoullam ardife, mute, hou, slet thicquyoust yuts, cof kerarbake an hit with:\n",
            "Bichs eniccexcut of swith thand?\n",
            "\n",
            "Whe sod Edar?\n",
            "\n",
            "OSell,\n",
            "aitht youf thimn!\n",
            "Whate ard ind me ull maco bnow\n",
            "\n",
            "Mlies.\n",
            "\n",
            "Iillf, as in\n",
            "Anwed buthen?\n",
            "Sefns why wed\n",
            "Ho this hu methent ward optirn; ute ous tith tul bererthes tene,\n",
            "Af'm'd my tall of bakd, aike low lin?\n",
            "\n",
            "Whem den frousmestus,\n",
            "Whis oold-, pull this,\n",
            "Thith coy, col to warce!\n",
            "Thy wy Rhat.\n",
            "\n",
            "OCINGEUSORY:\n",
            "I thre loug uncivee ove at acitht ord\n",
            "umy,\n",
            "As doth ballbartett a oullak or akor'y as he st; of din antyn with, bre,\n",
            "I'ng;'lly.\n",
            "Wortice santtun. Coply ith CE\n",
            "#########-----------------------------------------------------------------------------------------------------------------------------###### \n",
            " \n",
            "\n",
            "REFERENCE SUMMARY: \n",
            "First Citizen:\n",
            "Before we proceed any further, hear\n",
            "\n",
            "GENERATED SUMMARY: \n",
            "&LNOVEN:\n",
            "ONTEMS:\n",
            "Anm:\n",
            "Thall foakle tand oll adllie\n",
            "\n",
            "ROUGE_SCORES: \n",
            "\n",
            "{'rouge-1': {'r': 0.10426540284360189, 'p': 0.07051282051282051, 'f': 0.08413001430692933}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.10426540284360189, 'p': 0.07051282051282051, 'f': 0.08413001430692933}}\n"
          ]
        }
      ]
    }
  ]
}