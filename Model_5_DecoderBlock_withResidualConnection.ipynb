{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZl0NXVBjSHN",
        "outputId": "318a3b7c-d938-447f-9254-95f93ccc3882"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model 5: Merging everything into a block\n",
        "\n",
        "* Here we take this from the transformer architecture, where we have a encoder and decoder block with cross attention but we not have a a cross attention for now. Our bigram model will have a block which contain all the layers from above model but this will make trouble our model in optimization hence we call **`Residual Connections`** Our saviour beacause then backpropogation can easily happen during the optimization."
      ],
      "metadata": {
        "id": "QkFCNDKskh2K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nRuj35E2ih3O"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,num_head,head_size):\n",
        "      super().__init__()\n",
        "      self.head=nn.ModuleList([Head(head_size) for _ in range(num_head)])\n",
        "      #Adding a projection\n",
        "      self.proj=nn.Linear(n_embd,n_embd)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "      output=torch.cat([h(x) for h in self.head],dim=-1) # AS THE CONCAT TO THE CHANNEL SIDE SO DIM=-1(C)\n",
        "      output=self.proj(output)\n",
        "      return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net=nn.Sequential(\n",
        "        nn.Linear(n_embd,4* n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4* n_embd,n_embd)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self,n_embd,n_heads):\n",
        "    super().__init__()\n",
        "    self.con_head=MultiHeadAttention(n_heads,n_embd//n_heads)\n",
        "    self.ffdnn=FeedForward(n_embd)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "      x=x+self.con_head(x)\n",
        "      x=x+self.ffdnn(x)\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from rouge import Rouge\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences\n",
        "block_size = 8 # what is the maximum context length\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "n_head = 4\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "with open('/content/input (1).txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        perplexities = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss, perplexity = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "            perplexities[k] = perplexity.item() if perplexity is not None else float('nan')\n",
        "        out[f'{split}_loss'] = losses.mean()\n",
        "        out[f'{split}_perplexity'] = perplexities.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self,head_size):\n",
        "      super().__init__()\n",
        "      self.key=nn.Linear(n_embd,head_size,bias=False) ##Corresponds to the embedding layer, (see BTC shape here C is the embedding vector)\n",
        "      self.query=nn.Linear(n_embd,head_size,bias=False)\n",
        "      self.value=nn.Linear(n_embd,head_size,bias=False)\n",
        "      #Here tril is not the part of nn module to we add tril as buffer, meaning our custom function\n",
        "      self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size))) ##we made an upper triangle matrix above of shape T,T (8,8)\n",
        "\n",
        "#Forward process for the computation here\n",
        "    def forward(self,x):\n",
        "      B,T,C=x.shape\n",
        "      k=self.key(x)\n",
        "      q=self.query(x)\n",
        "\n",
        "      weight=q @ k.transpose(-2,-1) * C**-0.5\n",
        "      weight=weight.masked_fill(self.tril[:T,:T]==0,float('-inf'))\n",
        "      weight=F.softmax(weight,dim=-1)\n",
        "      v=self.value(x)\n",
        "      output=weight@v\n",
        "      return output\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,num_head,head_size):\n",
        "      super().__init__()\n",
        "      self.head=nn.ModuleList([Head(head_size) for _ in range(num_head)])\n",
        "      #Adding a projection\n",
        "      self.proj=nn.Linear(n_embd,n_embd)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "      output=torch.cat([h(x) for h in self.head],dim=-1) # AS THE CONCAT TO THE CHANNEL SIDE SO DIM=-1(C)\n",
        "      output=self.proj(output)\n",
        "      return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net=nn.Sequential(\n",
        "        nn.Linear(n_embd,4* n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4* n_embd,n_embd)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.net(x)\n",
        "\n",
        "'''\n",
        "Adding the block here: with residual conections to optimzie and make GD faster.\n",
        "Looking into the paper of the residual connection. basically adding the initial feature to lmake the model\n",
        "learn the forgotten features etc.\n",
        "'''\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self,n_embd,n_heads):\n",
        "    super().__init__()\n",
        "    self.con_head=MultiHeadAttention(n_heads,n_embd//n_heads)\n",
        "    self.ffdnn=FeedForward(n_embd)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "      x=x+self.con_head(x)\n",
        "      x=x+self.ffdnn(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "\n",
        "# super simple bigram model-5\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks=nn.Sequential(\n",
        "            Block(n_embd,n_heads=4),\n",
        "            Block(n_embd,n_heads=4),\n",
        "            Block(n_embd,n_heads=4),\n",
        "\n",
        "        )\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb =self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb =self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x =tok_emb + pos_emb # (B,T,C)\n",
        "        x=self.blocks(x)\n",
        "        logits =self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss=None\n",
        "            perplexity=None\n",
        "        else:\n",
        "            B,T,C =logits.shape\n",
        "            logits=logits.view(B*T, C)\n",
        "            targets=targets.view(B*T)\n",
        "            loss=F.cross_entropy(logits, targets)\n",
        "            probabilities=F.softmax(logits, dim=-1)\n",
        "            log_probabilities=torch.log(probabilities)\n",
        "            log_prob=torch.gather(log_probabilities,1,targets.unsqueeze(1)).squeeze(1)\n",
        "            N=targets.size(0)\n",
        "            perplexity=torch.exp(-torch.sum(log_prob)/N)\n",
        "\n",
        "        return logits, loss,perplexity\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond =idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss,perplexity = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits =logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs =F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next =torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx =torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train_loss']:.4f}, val loss {losses['val_loss']:.4f}, PERPLEX_TRAIN {losses['train_perplexity']:.4f} \"+\n",
        "              f\"PERPLEX_VAL {losses['val_perplexity']:.4f}\"\n",
        "        )\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss,perplexity = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n",
        "\n",
        "print(\"#########-----------------------------------------------------------------------------------------------------------------------------###### \\n \\n\")\n",
        "rouge = Rouge()\n",
        "generated_summaries = []\n",
        "context = torch.tensor([[4,24,26]], dtype=torch.long, device=device)\n",
        "summary= decode(m.generate(context, max_new_tokens=2000)[0].tolist())\n",
        "generated_summaries.append(summary)\n",
        "reference_summaries = []\n",
        "reference_summaries.append(text[:2000])\n",
        "print(\"REFERENCE SUMMARY: \\n\"+str(reference_summaries[0][:50])+\"\\n\")\n",
        "print(\"GENERATED SUMMARY: \\n\"+str(generated_summaries[0][:50])+\"\\n\")\n",
        "print(\"ROUGE_SCORES: \\n\")\n",
        "rouge_scores = rouge.get_scores(generated_summaries, reference_summaries)\n",
        "for score in rouge_scores:\n",
        "    print(score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8DoiEMLk3yX",
        "outputId": "0ddb9a94-c128-4eeb-a4e3-acd0fc20ea3f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.041921 M parameters\n",
            "step 0: train loss 4.6255, val loss 4.6233, PERPLEX_TRAIN 102.2762 PERPLEX_VAL 102.0387\n",
            "step 500: train loss 2.3882, val loss 2.3847, PERPLEX_TRAIN 10.9336 PERPLEX_VAL 10.8899\n",
            "step 1000: train loss 2.2709, val loss 2.2692, PERPLEX_TRAIN 9.7301 PERPLEX_VAL 9.7138\n",
            "step 1500: train loss 2.1883, val loss 2.2112, PERPLEX_TRAIN 8.9588 PERPLEX_VAL 9.1695\n",
            "step 2000: train loss 2.1477, val loss 2.1837, PERPLEX_TRAIN 8.6005 PERPLEX_VAL 8.9189\n",
            "step 2500: train loss 2.1065, val loss 2.1514, PERPLEX_TRAIN 8.2526 PERPLEX_VAL 8.6327\n",
            "step 3000: train loss 2.0704, val loss 2.1444, PERPLEX_TRAIN 7.9618 PERPLEX_VAL 8.5863\n",
            "step 3500: train loss 2.0624, val loss 2.1206, PERPLEX_TRAIN 7.8964 PERPLEX_VAL 8.3726\n",
            "step 4000: train loss 2.0281, val loss 2.1100, PERPLEX_TRAIN 7.6300 PERPLEX_VAL 8.2834\n",
            "step 4500: train loss 2.0040, val loss 2.1054, PERPLEX_TRAIN 7.4468 PERPLEX_VAL 8.2496\n",
            "step 4999: train loss 1.9998, val loss 2.0820, PERPLEX_TRAIN 7.4145 PERPLEX_VAL 8.0642\n",
            "\n",
            "Row duke be greford,\n",
            "What prach-deay loved, of persack'd the your farow, dide a cebad, bowlast be had not much man mertheir chere! That be spart not done, this and your eir heare a tha for wice,\n",
            "Have mock'd nod you fam do be king up too ever tow, omme as to louds as theem which it?\n",
            "That I'll to doyou calle dumpere sover your not trued so'd goe a dees for light comeas' but of theirst they cwound must ouse:\n",
            "Be huminect os and I a copar'ds: besees morder have, crightesse a\n",
            "died,\n",
            "They slove boody\n",
            "To to peaced,\n",
            "And sowd,\n",
            "Self thouse parcust.\n",
            "\n",
            "JESLUM:\n",
            "Sall of musy for him,\n",
            "Hard ubinked cometten up the fort as you. Fent ab, foon men, in bit dote.\n",
            "\n",
            "GLORESOF'?\n",
            "This that wive tiss, agast a may with ager, all hears of as your his the,\n",
            "Mis heart.\n",
            "'Ting the sirness dear's\n",
            "sue-platst thesious sward good the all is the mullifimme at wards morest now romfulian:\n",
            "Har mage loed ced, shall.\n",
            "Is wave many a the ploberient: am grace, love ear:\n",
            "Mastises thing. this quearth fir, sof the for sow be these to kneik my prick humy as soule\n",
            "My am and is as say's then with.\n",
            "Nor foe.\n",
            "\n",
            "GLOUMEONTENGULE:\n",
            "When we, merloset.\n",
            "\n",
            "LORARWIO:\n",
            "Jy this Lord kink,\n",
            "Wefire, and the, frome were somes.\n",
            "\n",
            "The men amfash ints.\n",
            "\n",
            "LARGELUS:\n",
            "Were but had:\n",
            "Idave plount the fie, to nlow welcesser,\n",
            "Fall no'd,\n",
            "And green taus,\n",
            "Their her on sumpy your begal I my his faced of fser. I of the quags and tasdett: I muse,\n",
            "Thum, now of I mide a were would eowell gestientle a ther of he how thime: would us as no'd of ganted came, you seithirn it fuld wift!\n",
            "And for mish in all as but am enly:\n",
            "Sowill:\n",
            "The live: is famino, say, his tind dufert we shall a pass eirs,\n",
            "Their, of be, not crack! I meas not-le set I'll hady ure in row tis this iugl?\n",
            "Kre, sausies Your hooroughter me imper not modable, gar; wead wartust folles, your stold the sogiend with\n",
            "cound ows anyst batiast and say wought dish and on'd ward; on made you kide cilly muster, parceservine.\n",
            "\n",
            "Sempeerness to couset. This, po man the othewardst dided us willse pello, foor be.\n",
            "\n",
            "DUKE GST\n",
            "#########-----------------------------------------------------------------------------------------------------------------------------###### \n",
            " \n",
            "\n",
            "REFERENCE SUMMARY: \n",
            "First Citizen:\n",
            "Before we proceed any further, hear\n",
            "\n",
            "GENERATED SUMMARY: \n",
            "&LND:\n",
            "Herever bady, say seren men the and done, th\n",
            "\n",
            "ROUGE_SCORES: \n",
            "\n",
            "{'rouge-1': {'r': 0.2037914691943128, 'p': 0.15808823529411764, 'f': 0.17805382530749428}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.1895734597156398, 'p': 0.14705882352941177, 'f': 0.16563146505904708}}\n"
          ]
        }
      ]
    }
  ]
}