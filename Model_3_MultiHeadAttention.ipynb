{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZl0NXVBjSHN",
        "outputId": "318a3b7c-d938-447f-9254-95f93ccc3882"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRuj35E2ih3O",
        "outputId": "161da989-5f36-436c-bb63-09f1f0fce971"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.007553 M parameters\n",
            "step 0: train loss 4.2227, val loss 4.2226, PERPLEX_TRAIN 68.2249 PERPLEX_VAL 68.2163\n",
            "step 500: train loss 2.6592, val loss 2.6733, PERPLEX_TRAIN 14.3368 PERPLEX_VAL 14.5413\n",
            "step 1000: train loss 2.4980, val loss 2.5064, PERPLEX_TRAIN 12.2041 PERPLEX_VAL 12.3049\n",
            "step 1500: train loss 2.4291, val loss 2.4349, PERPLEX_TRAIN 11.3828 PERPLEX_VAL 11.4621\n",
            "step 2000: train loss 2.3716, val loss 2.3844, PERPLEX_TRAIN 10.7542 PERPLEX_VAL 10.8932\n",
            "step 2500: train loss 2.3417, val loss 2.3561, PERPLEX_TRAIN 10.4407 PERPLEX_VAL 10.5836\n",
            "step 3000: train loss 2.3149, val loss 2.3347, PERPLEX_TRAIN 10.1666 PERPLEX_VAL 10.3650\n",
            "step 3500: train loss 2.2918, val loss 2.3171, PERPLEX_TRAIN 9.9207 PERPLEX_VAL 10.1877\n",
            "step 4000: train loss 2.2895, val loss 2.2868, PERPLEX_TRAIN 9.9144 PERPLEX_VAL 9.8793\n",
            "step 4500: train loss 2.2748, val loss 2.2858, PERPLEX_TRAIN 9.7640 PERPLEX_VAL 9.8639\n",
            "step 4999: train loss 2.2675, val loss 2.2857, PERPLEX_TRAIN 9.6889 PERPLEX_VAL 9.8755\n",
            "\n",
            "Shiserrs.\n",
            "ANTE:\n",
            "Ther strestr,\n",
            "Go sorncell wavoufand'd mey wites, ingd?\n",
            "Ofer eedroee o, lal be for day.\n",
            "\n",
            "CINGOUKOD:\n",
            "Beent, they noould.\n",
            "\n",
            "FOFeigf ann he ceaket ut foore ethingt. Lickou Wity wiletis I I welthe, gre breancellaisser'ct dines ab, shall\n",
            "BIfor thoultans fall tor ber-R\n",
            "SAPIOLO:\n",
            "Aseclode, Iuast, ou ing dimrom.\n",
            "\n",
            "Mnti, cledencirut lich whant my! I theesendsifor ou he YOngs I thar:' coupers?\n",
            "Wats\n",
            "Moceou they his stere your Bure healnd;\n",
            "Srienatay\n",
            "Marlif!\n",
            "Anut I a sowol: wou hout bllant\n",
            "re ther foould late s, you,\n",
            "Feald thove falday an\n",
            "Server'wse ow he\n",
            "Junghongooug shesed umerd.\n",
            "\n",
            "LINTALIH:\n",
            "Cill or goont by beate:\n",
            "Thath I swit vo pasers frid: thoust too,\n",
            "Thitherovess ipne; shoull pripen berllesikn, eease?\n",
            "Nw;\n",
            "But ball.\n",
            "\n",
            "SES:\n",
            "Coueretere, do;\n",
            "Angthernt omen, I woilesptinf slt shigace,\n",
            "Gu, cars by hapbard sit by that aay siviord mell therrense,\n",
            "Se poin carky.\n",
            "\n",
            "Clit weag-se\n",
            "Toem'ss art wre to an thushtit samendd, to imerterod? ou iwirod wour I croveriurt dousen! me; walld'llcr'd freve boulsechilcce, yournias leand\n",
            "joat locald by men, the;\n",
            "That clea! levinl-d\n",
            "Thri, lee badvere,\n",
            "Wo?\n",
            "\n",
            "I her, malg.\n",
            "\n",
            "\n",
            "Mard for shan loully sesald is thepes-decameakniceraing sor he mjet! owne? wit.\n",
            "\n",
            "ICoteg noth and?\n",
            "\n",
            "GUESGLHTer?\n",
            "\n",
            "Thine, shatard, to, Go by st wher;\n",
            "An, frany whim furt.\n",
            "\n",
            "Theldcee me ity antult to bat aet alene?\n",
            "\n",
            "HTarm: tis art I jet.\n",
            "That rouse not us onocard-tucchby what in dis,\n",
            "Thatur gep of hun, go dellir ofr ug sst meye sle\n",
            "Thoustoorme; ther?\n",
            "\n",
            "linr bbopind to I asie thald to iss avey is Hgith, dome.\n",
            "\n",
            "QUwime foulish wise st: wexth shis if barce I thoutrne breeyaim hay sesil ho, ant ald; k, why it bit ot To Clivest sod?\n",
            "\n",
            "Angerords uvend I IA'd liknfoldifhed an Edis silvef youmed ithre,' annomer so of whe,\n",
            "Ont, direde, the you thand nos surald the, thou theiss, you ivo thaik alcry to not me; shain sod weand do we athe ighs we Ife whill hou veear to gre suwe.\n",
            "Yog wierlold,\n",
            "ma, inet; thite, treday wovee forow cath!\n",
            "I REAMCIANTOR:\n",
            "Pell of theime fltuse, ave at oun welardowtl uns \n",
            "#########-----------------------------------------------------------------------------------------------------------------------------###### \n",
            " \n",
            "\n",
            "REFERENCE SUMMARY: \n",
            "First Citizen:\n",
            "Before we proceed any further, hear\n",
            "\n",
            "GENERATED SUMMARY: \n",
            "&LNE:\n",
            "You forsteranl coo.\n",
            "\n",
            "Whaik: mou wo itarlow t\n",
            "\n",
            "ROUGE_SCORES: \n",
            "\n",
            "{'rouge-1': {'r': 0.06635071090047394, 'p': 0.04590163934426229, 'f': 0.054263561057403834}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.061611374407582936, 'p': 0.04262295081967213, 'f': 0.050387592065155806}}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from rouge import Rouge\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences\n",
        "block_size = 8 # what is the maximum context\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "n_head = 4\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "with open('/content/input (1).txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        perplexities = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss, perplexity = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "            perplexities[k] = perplexity.item() if perplexity is not None else float('nan')\n",
        "        out[f'{split}_loss'] = losses.mean()\n",
        "        out[f'{split}_perplexity'] = perplexities.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self,head_size):\n",
        "      super().__init__()\n",
        "      self.key=nn.Linear(n_embd,head_size,bias=False) ##Corresponds to the embedding layer, (see BTC shape here C is the embedding vector)\n",
        "      self.query=nn.Linear(n_embd,head_size,bias=False)\n",
        "      self.value=nn.Linear(n_embd,head_size,bias=False)\n",
        "      #Here tril is not the part of nn module to we add tril as buffer, meaning our custom function\n",
        "      self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size))) ##we made an upper triangle matrix above of shape T,T (8,8)\n",
        "\n",
        "#Forward process for the computation here\n",
        "    def forward(self,x):\n",
        "      B,T,C=x.shape\n",
        "      k=self.key(x)\n",
        "      q=self.query(x)\n",
        "\n",
        "      weight=q @ k.transpose(-2,-1) * C**-0.5\n",
        "      weight=weight.masked_fill(self.tril[:T,:T]==0,float('-inf'))\n",
        "      weight=F.softmax(weight,dim=-1)\n",
        "      v=self.value(x)\n",
        "      output=weight@v\n",
        "      return output\n",
        "\n",
        "\n",
        "'''We need to find multiple relation between the words herer , so we use multi-head-attention.\n",
        " We give 4 attention as our output to capture different relation between the words and each attention giving out 8 block of words and concenate\n",
        "them to give an output of 32 which will be equal to our n_embeddings '''\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,num_head,head_size):\n",
        "      super().__init__()\n",
        "      self.head=nn.ModuleList([Head(head_size) for _ in range(num_head)])\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "      output=torch.cat([h(x) for h in self.head],dim=-1) # AS THE CONCAT TO THE CHANNEL SIDE SO DIM=-1(C)\n",
        "      return output\n",
        "\n",
        "\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.con_head=MultiHeadAttention(4,n_embd//4) # 4 Heads concat to 32 (=n_embd vector)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x =tok_emb + pos_emb # (B,T,C)\n",
        "        x =self.con_head(x) # Implementing the self_attention\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss=None\n",
        "            perplexity=None\n",
        "        else:\n",
        "            B,T,C =logits.shape\n",
        "            logits=logits.view(B*T, C)\n",
        "            targets=targets.view(B*T)\n",
        "            loss=F.cross_entropy(logits, targets)\n",
        "            probabilities=F.softmax(logits, dim=-1)\n",
        "            log_probabilities=torch.log(probabilities)\n",
        "            log_prob=torch.gather(log_probabilities,1,targets.unsqueeze(1)).squeeze(1)\n",
        "            N=targets.size(0)\n",
        "            perplexity=torch.exp(-torch.sum(log_prob)/N)\n",
        "\n",
        "        return logits, loss,perplexity\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond =idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss,perplexity = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits =logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs =F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next =torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx =torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train_loss']:.4f}, val loss {losses['val_loss']:.4f}, PERPLEX_TRAIN {losses['train_perplexity']:.4f} \"+\n",
        "              f\"PERPLEX_VAL {losses['val_perplexity']:.4f}\"\n",
        "        )\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss,perplexity = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n",
        "\n",
        "print(\"#########-----------------------------------------------------------------------------------------------------------------------------###### \\n \\n\")\n",
        "rouge = Rouge()\n",
        "generated_summaries = []\n",
        "context = torch.tensor([[4,24,26]], dtype=torch.long, device=device)\n",
        "summary= decode(m.generate(context, max_new_tokens=2000)[0].tolist())\n",
        "generated_summaries.append(summary)\n",
        "reference_summaries = []\n",
        "reference_summaries.append(text[:2000])\n",
        "print(\"REFERENCE SUMMARY: \\n\"+str(reference_summaries[0][:50])+\"\\n\")\n",
        "print(\"GENERATED SUMMARY: \\n\"+str(generated_summaries[0][:50])+\"\\n\")\n",
        "print(\"ROUGE_SCORES: \\n\")\n",
        "rouge_scores = rouge.get_scores(generated_summaries, reference_summaries)\n",
        "for score in rouge_scores:\n",
        "    print(score)"
      ]
    }
  ]
}