{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLXHzkVRg7Tp",
        "outputId": "a96a3dde-4baf-4733-fd51-45dbb7fe86dd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from rouge import Rouge\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences\n",
        "block_size = 8 # what is the maximum context length\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "n_head = 4\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "with open('/content/input (1).txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        perplexities = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss, perplexity = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "            perplexities[k] = perplexity.item() if perplexity is not None else float('nan')\n",
        "        out[f'{split}_loss'] = losses.mean()\n",
        "        out[f'{split}_perplexity'] = perplexities.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self,head_size):\n",
        "      super().__init__()\n",
        "      self.key=nn.Linear(n_embd,head_size,bias=False) ##Corresponds to the embedding layer, (see BTC shape here C is the embedding vector)\n",
        "      self.query=nn.Linear(n_embd,head_size,bias=False)\n",
        "      self.value=nn.Linear(n_embd,head_size,bias=False)\n",
        "      #Here tril is not the part of nn module to we add tril as buffer, meaning our custom function\n",
        "      self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size))) ##we made an upper triangle matrix above of shape T,T (8,8)\n",
        "\n",
        "#Forward process for the computation here\n",
        "    def forward(self,x):\n",
        "      B,T,C=x.shape\n",
        "      k=self.key(x)\n",
        "      q=self.query(x)\n",
        "\n",
        "      weight=q @ k.transpose(-2,-1) * C**-0.5\n",
        "      weight=weight.masked_fill(self.tril[:T,:T]==0,float('-inf'))\n",
        "      weight=F.softmax(weight,dim=-1)\n",
        "      v=self.value(x)\n",
        "      output=weight@v\n",
        "      return output\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,num_head,head_size):\n",
        "      super().__init__()\n",
        "      self.head=nn.ModuleList([Head(head_size) for _ in range(num_head)])\n",
        "      #Adding a projection\n",
        "      self.proj=nn.Linear(n_embd,n_embd)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "      output=torch.cat([h(x) for h in self.head],dim=-1) # AS THE CONCAT TO THE CHANNEL SIDE SO DIM=-1(C)\n",
        "      output=self.proj(output)\n",
        "      return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net=nn.Sequential(\n",
        "        nn.Linear(n_embd,4* n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4* n_embd,n_embd)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.net(x)\n",
        "\n",
        "'''\n",
        "Adding the block here: with residual conections to optimzie and make GD faster.\n",
        "Looking into the paper of the residual connection. basically adding the initial feature to lmake the model\n",
        "learn the forgotten features etc.\n",
        "'''\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self,n_embd,n_heads):\n",
        "    super().__init__()\n",
        "    self.con_head=MultiHeadAttention(n_heads,n_embd//n_heads)\n",
        "    self.ffdnn=FeedForward(n_embd)\n",
        "    self.ln_1=nn.LayerNorm(n_embd)\n",
        "    self.ln_2=nn.LayerNorm(n_embd)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "      x=x+self.con_head(self.ln_1(x))\n",
        "      x=x+self.ffdnn(self.ln_2(x))\n",
        "      return x\n",
        "\n",
        "\n",
        "\n",
        "# super simple bigram model-5\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks=nn.Sequential(\n",
        "            Block(n_embd,n_heads=4),\n",
        "            Block(n_embd,n_heads=4),\n",
        "            Block(n_embd,n_heads=4),\n",
        "            nn.LayerNorm(n_embd),\n",
        "\n",
        "        )\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb +pos_emb # (B,T,C)\n",
        "        x=self.blocks(x)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss=None\n",
        "            perplexity=None\n",
        "        else:\n",
        "            B,T,C =logits.shape\n",
        "            logits=logits.view(B*T, C)\n",
        "            targets=targets.view(B*T)\n",
        "            loss=F.cross_entropy(logits, targets)\n",
        "            probabilities=F.softmax(logits, dim=-1)\n",
        "            log_probabilities=torch.log(probabilities)\n",
        "            log_prob=torch.gather(log_probabilities,1,targets.unsqueeze(1)).squeeze(1)\n",
        "            N=targets.size(0)\n",
        "            perplexity=torch.exp(-torch.sum(log_prob)/N)\n",
        "\n",
        "        return logits, loss,perplexity\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond =idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss,perplexity = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits =logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs =F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next =torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx =torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train_loss']:.4f}, val loss {losses['val_loss']:.4f}, PERPLEX_TRAIN {losses['train_perplexity']:.4f} \"+\n",
        "              f\"PERPLEX_VAL {losses['val_perplexity']:.4f}\"\n",
        "        )\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss,perplexity = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n",
        "\n",
        "print(\"#########-----------------------------------------------------------------------------------------------------------------------------###### \\n \\n\")\n",
        "rouge = Rouge()\n",
        "generated_summaries = []\n",
        "context = torch.tensor([[4,24,26]], dtype=torch.long, device=device)\n",
        "summary= decode(m.generate(context, max_new_tokens=2000)[0].tolist())\n",
        "generated_summaries.append(summary)\n",
        "reference_summaries = []\n",
        "reference_summaries.append(text[:2000])\n",
        "print(\"REFERENCE SUMMARY: \\n\"+str(reference_summaries[0][:50])+\"\\n\")\n",
        "print(\"GENERATED SUMMARY: \\n\"+str(generated_summaries[0][:50])+\"\\n\")\n",
        "print(\"ROUGE_SCORES: \\n\")\n",
        "rouge_scores = rouge.get_scores(generated_summaries, reference_summaries)\n",
        "for score in rouge_scores:\n",
        "    print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNwPxYEjlpdt",
        "outputId": "aeb7433a-2d9f-464f-ff8c-49b49f53d19e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.042369 M parameters\n",
            "step 0: train loss 4.3103, val loss 4.3097, PERPLEX_TRAIN 74.5176 PERPLEX_VAL 74.4740\n",
            "step 500: train loss 2.3998, val loss 2.4008, PERPLEX_TRAIN 11.0578 PERPLEX_VAL 11.0641\n",
            "step 1000: train loss 2.2644, val loss 2.2660, PERPLEX_TRAIN 9.6623 PERPLEX_VAL 9.6832\n",
            "step 1500: train loss 2.1670, val loss 2.1896, PERPLEX_TRAIN 8.7668 PERPLEX_VAL 8.9721\n",
            "step 2000: train loss 2.1318, val loss 2.1680, PERPLEX_TRAIN 8.4632 PERPLEX_VAL 8.7813\n",
            "step 2500: train loss 2.0813, val loss 2.1305, PERPLEX_TRAIN 8.0435 PERPLEX_VAL 8.4553\n",
            "step 3000: train loss 2.0526, val loss 2.1242, PERPLEX_TRAIN 7.8206 PERPLEX_VAL 8.4115\n",
            "step 3500: train loss 2.0436, val loss 2.1059, PERPLEX_TRAIN 7.7490 PERPLEX_VAL 8.2497\n",
            "step 4000: train loss 2.0137, val loss 2.0946, PERPLEX_TRAIN 7.5212 PERPLEX_VAL 8.1520\n",
            "step 4500: train loss 1.9908, val loss 2.0944, PERPLEX_TRAIN 7.3469 PERPLEX_VAL 8.1607\n",
            "step 4999: train loss 1.9801, val loss 2.0589, PERPLEX_TRAIN 7.2689 PERPLEX_VAL 7.8780\n",
            "\n",
            "Row duke bergeer?\n",
            "\n",
            "VIOLTULANG'T:\n",
            "I ye: by cloousers,\n",
            "Et thy lowery\n",
            "Sho lad theat shad, brive do the that a magent. Of balieb hear! The of these way is pearter and not\n",
            "woer leew leat that sir?\n",
            "\n",
            "CLANTOLA:\n",
            "I' youd you fal do broke. ancallooe, of Theremy\n",
            "The to looks and calmoned so, but\n",
            "Senver.'\n",
            "A'Toy fearl nor.\n",
            "Wheree,\n",
            "That alent,\n",
            "Lef dusery goe a my sid.\n",
            "\n",
            "JULEORCUS:\n",
            "Lord,--\n",
            "GANGONUETIO: you sufil. My know the huminect os and.\n",
            "\n",
            "COLY ELOMENB:\n",
            "The goodds, have, Or, beesterath,\n",
            "Thus grool! I so dyecher\n",
            "Boble do but sowdly sof thous wivolys the wouldsol? sbeous ty for him,\n",
            "Hare ubly it courter and the foe. Ore'-will no go, doneful heipe is doign all lord,\n",
            "It.\n",
            "What that I, tisto agasty, make your geathed whries good his velied the'd\n",
            "But as of'll, long, if as colengs\n",
            "stear\n",
            "Ovet these withbrest\n",
            "With her.\n",
            "\n",
            "ISICKINA:\n",
            "A gramel not beses heave now romful:\n",
            "Away? Suriesl en codo sho to casty witt.\n",
            "\n",
            "ERLALEO!\n",
            "AD IV:\n",
            "What grace, love ear againg\n",
            "roths fall, dones, it remity of her,\n",
            "The dive\n",
            "What up, suplik word the hear a punion\n",
            "My am all ind the feceengallies.\n",
            "Not fallings you him tatemf heis he mehat his in we.\n",
            "DaEk yet my Lord kixt,\n",
            "We if leadpst dave the were seed fill too fiarlesh interear.\n",
            "Good iug prome bedany, a so, of a the fie, to nlog welless you the no'd,\n",
            "And green tather seir was on sur Mure have sordest him.\n",
            "\n",
            "NUERor fser. I your flattes's but, ditry rooughther I donce.\n",
            "\n",
            "If if an in the twell, wlll'd the meethink I of he how not annes.\n",
            "Wiuse, may her thent sacks, cour beithirn i prusence to that !\n",
            "\n",
            "LUCAPOLAPUS:\n",
            "By time you you donet:\n",
            "The in tioker, mist, say, hy hady mous\n",
            "At well: hims of soldry,\n",
            "Their. KING VI:\n",
            "Firsed as will he to will reched a your on i, wation him inglat rewis? agess trojuserces as the imve in that a lance's; wead wars it folles,\n",
            "And that to bessegray is;\n",
            "My canceres ands blattake and say would frish and\n",
            "To vawn mast pall I but seet if dembled on some ead in.\n",
            "\n",
            "SCORYDOUCERSTEL:\n",
            "If then is, po of come our warest did my sore.\n",
            "\n",
            "SOMISCETIO:\n",
            "So crasiefore.\n",
            "\n",
            "#########-----------------------------------------------------------------------------------------------------------------------------###### \n",
            " \n",
            "\n",
            "REFERENCE SUMMARY: \n",
            "First Citizen:\n",
            "Before we proceed any further, hear\n",
            "\n",
            "GENERATED SUMMARY: \n",
            "&LNAy to;\n",
            "Here a pliest seick of himshand done, th\n",
            "\n",
            "ROUGE_SCORES: \n",
            "\n",
            "{'rouge-1': {'r': 0.1943127962085308, 'p': 0.148014440433213, 'f': 0.16803278197670332}, 'rouge-2': {'r': 0.002967359050445104, 'p': 0.0027548209366391185, 'f': 0.0028571378640495425}, 'rouge-l': {'r': 0.1848341232227488, 'p': 0.1407942238267148, 'f': 0.1598360606652279}}\n"
          ]
        }
      ]
    }
  ]
}