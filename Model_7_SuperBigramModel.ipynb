{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZl0NXVBjSHN",
        "outputId": "a06b2878-b339-4edc-c10a-e3f94a334066"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRuj35E2ih3O",
        "outputId": "1a83d446-38f5-4c97-80f6-4548ba18db96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.788929 M parameters\n",
            "step 0: train loss 4.2849, val loss 4.2823, PERPLEX_TRAIN 72.5948 PERPLEX_VAL 72.4087\n",
            "Model checkpoint saved at iteration 0.\n",
            "step 100: train loss 2.4585, val loss 2.4788, PERPLEX_TRAIN 11.6880 PERPLEX_VAL 11.9286\n",
            "Model checkpoint saved at iteration 100.\n",
            "step 200: train loss 2.3426, val loss 2.3912, PERPLEX_TRAIN 10.4093 PERPLEX_VAL 10.9273\n",
            "Model checkpoint saved at iteration 200.\n",
            "step 300: train loss 2.0551, val loss 2.1426, PERPLEX_TRAIN 7.8084 PERPLEX_VAL 8.5231\n",
            "Model checkpoint saved at iteration 300.\n",
            "step 400: train loss 1.8359, val loss 1.9604, PERPLEX_TRAIN 6.2720 PERPLEX_VAL 7.1033\n",
            "Model checkpoint saved at iteration 400.\n",
            "step 500: train loss 1.6761, val loss 1.8353, PERPLEX_TRAIN 5.3456 PERPLEX_VAL 6.2683\n",
            "Model checkpoint saved at iteration 500.\n",
            "step 600: train loss 1.5668, val loss 1.7468, PERPLEX_TRAIN 4.7921 PERPLEX_VAL 5.7379\n",
            "Model checkpoint saved at iteration 600.\n",
            "step 700: train loss 1.4915, val loss 1.6832, PERPLEX_TRAIN 4.4445 PERPLEX_VAL 5.3847\n",
            "Model checkpoint saved at iteration 700.\n",
            "step 800: train loss 1.4341, val loss 1.6422, PERPLEX_TRAIN 4.1965 PERPLEX_VAL 5.1686\n",
            "Model checkpoint saved at iteration 800.\n",
            "step 900: train loss 1.3870, val loss 1.6016, PERPLEX_TRAIN 4.0037 PERPLEX_VAL 4.9626\n",
            "Model checkpoint saved at iteration 900.\n",
            "step 1000: train loss 1.3482, val loss 1.5758, PERPLEX_TRAIN 3.8513 PERPLEX_VAL 4.8367\n",
            "Model checkpoint saved at iteration 1000.\n",
            "step 1100: train loss 1.3183, val loss 1.5503, PERPLEX_TRAIN 3.7379 PERPLEX_VAL 4.7146\n",
            "Model checkpoint saved at iteration 1100.\n",
            "step 1200: train loss 1.2934, val loss 1.5348, PERPLEX_TRAIN 3.6458 PERPLEX_VAL 4.6425\n",
            "Model checkpoint saved at iteration 1200.\n",
            "step 1300: train loss 1.2705, val loss 1.5207, PERPLEX_TRAIN 3.5634 PERPLEX_VAL 4.5769\n",
            "Model checkpoint saved at iteration 1300.\n",
            "step 1400: train loss 1.2459, val loss 1.5125, PERPLEX_TRAIN 3.4767 PERPLEX_VAL 4.5403\n",
            "Model checkpoint saved at iteration 1400.\n",
            "step 1500: train loss 1.2316, val loss 1.5127, PERPLEX_TRAIN 3.4272 PERPLEX_VAL 4.5413\n",
            "Model checkpoint saved at iteration 1500.\n",
            "step 1600: train loss 1.2136, val loss 1.5036, PERPLEX_TRAIN 3.3663 PERPLEX_VAL 4.5002\n",
            "Model checkpoint saved at iteration 1600.\n",
            "step 1700: train loss 1.1965, val loss 1.4940, PERPLEX_TRAIN 3.3090 PERPLEX_VAL 4.4567\n",
            "Model checkpoint saved at iteration 1700.\n",
            "step 1800: train loss 1.1826, val loss 1.4896, PERPLEX_TRAIN 3.2633 PERPLEX_VAL 4.4381\n",
            "Model checkpoint saved at iteration 1800.\n",
            "step 1900: train loss 1.1643, val loss 1.4782, PERPLEX_TRAIN 3.2040 PERPLEX_VAL 4.3872\n",
            "Model checkpoint saved at iteration 1900.\n",
            "step 2000: train loss 1.1514, val loss 1.4734, PERPLEX_TRAIN 3.1632 PERPLEX_VAL 4.3666\n",
            "Model checkpoint saved at iteration 2000.\n",
            "step 2100: train loss 1.1345, val loss 1.4852, PERPLEX_TRAIN 3.1101 PERPLEX_VAL 4.4185\n",
            "Model checkpoint saved at iteration 2100.\n",
            "step 2200: train loss 1.1250, val loss 1.4830, PERPLEX_TRAIN 3.0807 PERPLEX_VAL 4.4088\n",
            "Model checkpoint saved at iteration 2200.\n",
            "step 2300: train loss 1.1120, val loss 1.4864, PERPLEX_TRAIN 3.0408 PERPLEX_VAL 4.4235\n",
            "Model checkpoint saved at iteration 2300.\n",
            "step 2400: train loss 1.1004, val loss 1.4807, PERPLEX_TRAIN 3.0057 PERPLEX_VAL 4.3980\n",
            "Model checkpoint saved at iteration 2400.\n",
            "step 2500: train loss 1.0850, val loss 1.4901, PERPLEX_TRAIN 2.9598 PERPLEX_VAL 4.4397\n",
            "Model checkpoint saved at iteration 2500.\n",
            "step 2600: train loss 1.0721, val loss 1.4837, PERPLEX_TRAIN 2.9219 PERPLEX_VAL 4.4118\n",
            "Model checkpoint saved at iteration 2600.\n",
            "step 2700: train loss 1.0575, val loss 1.4869, PERPLEX_TRAIN 2.8796 PERPLEX_VAL 4.4262\n",
            "Model checkpoint saved at iteration 2700.\n",
            "step 2800: train loss 1.0479, val loss 1.4870, PERPLEX_TRAIN 2.8519 PERPLEX_VAL 4.4263\n",
            "Model checkpoint saved at iteration 2800.\n",
            "step 2900: train loss 1.0368, val loss 1.4978, PERPLEX_TRAIN 2.8205 PERPLEX_VAL 4.4749\n",
            "Model checkpoint saved at iteration 2900.\n",
            "step 3000: train loss 1.0243, val loss 1.4998, PERPLEX_TRAIN 2.7855 PERPLEX_VAL 4.4833\n",
            "Model checkpoint saved at iteration 3000.\n",
            "step 3100: train loss 1.0093, val loss 1.5044, PERPLEX_TRAIN 2.7442 PERPLEX_VAL 4.5046\n",
            "Model checkpoint saved at iteration 3100.\n",
            "step 3200: train loss 0.9992, val loss 1.5236, PERPLEX_TRAIN 2.7165 PERPLEX_VAL 4.5923\n",
            "Model checkpoint saved at iteration 3200.\n",
            "step 3300: train loss 0.9862, val loss 1.5314, PERPLEX_TRAIN 2.6813 PERPLEX_VAL 4.6280\n",
            "Model checkpoint saved at iteration 3300.\n",
            "step 3400: train loss 0.9722, val loss 1.5216, PERPLEX_TRAIN 2.6440 PERPLEX_VAL 4.5831\n",
            "Model checkpoint saved at iteration 3400.\n",
            "step 3500: train loss 0.9632, val loss 1.5270, PERPLEX_TRAIN 2.6204 PERPLEX_VAL 4.6081\n",
            "Model checkpoint saved at iteration 3500.\n",
            "step 3600: train loss 0.9507, val loss 1.5352, PERPLEX_TRAIN 2.5877 PERPLEX_VAL 4.6455\n",
            "Model checkpoint saved at iteration 3600.\n",
            "step 3700: train loss 0.9343, val loss 1.5268, PERPLEX_TRAIN 2.5458 PERPLEX_VAL 4.6068\n",
            "Model checkpoint saved at iteration 3700.\n",
            "step 3800: train loss 0.9249, val loss 1.5326, PERPLEX_TRAIN 2.5220 PERPLEX_VAL 4.6336\n",
            "Model checkpoint saved at iteration 3800.\n",
            "step 3900: train loss 0.9178, val loss 1.5552, PERPLEX_TRAIN 2.5041 PERPLEX_VAL 4.7391\n",
            "Model checkpoint saved at iteration 3900.\n",
            "step 4000: train loss 0.9040, val loss 1.5496, PERPLEX_TRAIN 2.4698 PERPLEX_VAL 4.7131\n",
            "Model checkpoint saved at iteration 4000.\n",
            "step 4100: train loss 0.8864, val loss 1.5697, PERPLEX_TRAIN 2.4267 PERPLEX_VAL 4.8092\n",
            "Model checkpoint saved at iteration 4100.\n",
            "step 4200: train loss 0.8808, val loss 1.5762, PERPLEX_TRAIN 2.4130 PERPLEX_VAL 4.8402\n",
            "Model checkpoint saved at iteration 4200.\n",
            "step 4300: train loss 0.8646, val loss 1.5827, PERPLEX_TRAIN 2.3742 PERPLEX_VAL 4.8729\n",
            "Model checkpoint saved at iteration 4300.\n",
            "step 4400: train loss 0.8532, val loss 1.5798, PERPLEX_TRAIN 2.3474 PERPLEX_VAL 4.8581\n",
            "Model checkpoint saved at iteration 4400.\n",
            "step 4500: train loss 0.8439, val loss 1.6036, PERPLEX_TRAIN 2.3258 PERPLEX_VAL 4.9756\n",
            "Model checkpoint saved at iteration 4500.\n",
            "step 4600: train loss 0.8288, val loss 1.6068, PERPLEX_TRAIN 2.2908 PERPLEX_VAL 4.9918\n",
            "Model checkpoint saved at iteration 4600.\n",
            "step 4700: train loss 0.8193, val loss 1.6123, PERPLEX_TRAIN 2.2691 PERPLEX_VAL 5.0184\n",
            "Model checkpoint saved at iteration 4700.\n",
            "step 4800: train loss 0.8095, val loss 1.6209, PERPLEX_TRAIN 2.2471 PERPLEX_VAL 5.0623\n",
            "Model checkpoint saved at iteration 4800.\n",
            "step 4900: train loss 0.7994, val loss 1.6184, PERPLEX_TRAIN 2.2245 PERPLEX_VAL 5.0499\n",
            "Model checkpoint saved at iteration 4900.\n",
            "step 4999: train loss 0.7818, val loss 1.6332, PERPLEX_TRAIN 2.1856 PERPLEX_VAL 5.1250\n",
            "Model checkpoint saved at iteration 4999.\n",
            "\n",
            "Behold to fight: I will see as I will come.\n",
            "But must I rest alone, I hope, or at thy groan;\n",
            "Where should mutiny in wonders, he must come\n",
            "To perfect by thy lips and all thy lie,\n",
            "Should not help; all there is elevate to help.\n",
            "I see the damned villains of state hence of it\n",
            "Which steed upon my soldiers.\n",
            "\n",
            "First Citizen:\n",
            "My great grief articular and tickle's care,\n",
            "That I have loved possessives, cracking in their estimation!\n",
            "You have misused, your bloody cousins\n",
            "Had more his oath from Lord Benetug, by\n",
            "Than should do worse than what might have to him.\n",
            "\n",
            "BENVOLIO:\n",
            "An honour's blessed to comfort mixt the rade.\n",
            "Be ruled, blade is bloody, rob oad, like honourable.\n",
            "\n",
            "LUCIO:\n",
            "From my marr's speech, fair blood? be oC, come, sir, low:\n",
            "I'll say to me would go what you did;\n",
            "Nothing but one dish, but not made prepaice,\n",
            "For some that you'ld fain smoop will not\n",
            "Intend anotheritituous to answer\n",
            "That with grossips it most not your pleasure.\n",
            "Go, get this hollow to your rude service,\n",
            "Proclaim not upon your purse. Your honour is\n",
            "Of her tongue, protector, that to the roor\n",
            "When on the nature, how is laid to,\n",
            "I taste the usurpies of breath, and breathes\n",
            "Red approbation! Wherein they have strongly restrain'd?\n",
            "\n",
            "Second Citizen:\n",
            "Pray, behold! then they call he's done.\n",
            "\n",
            "First Citizen:\n",
            "'Tis honesty; but O, he's a brother dead,\n",
            "How is that you have contemn'd ear them crown\n",
            "Upon our loss four sights; yet that his royalty,\n",
            "Forturing the duke secreceive the crown,\n",
            "Cymbaching the queen, the rabble soul vow\n",
            "And he show'd it going: her husband mark worthin,\n",
            "He writed very fair, for love\n",
            "An one but fell'd actions. Harward ha!\n",
            "W is he not Crown: Come, home you hither?\n",
            "\n",
            "EXETER:\n",
            "Unconducted already for me;\n",
            "And, he will come to pardon with his name.\n",
            "\n",
            "KING HENRY VI:\n",
            "Ant:\n",
            "If good news, heartily were blind as now were\n",
            "Tybalt us with the shroud and weeping scorn him.\n",
            "With wiorcing thy sorrow-for worth or wrong,\n",
            "I warrant, he's a nose but thy day's dearth.\n",
            "\n",
            "KING HENRY VI:\n",
            "Why, as you live to be from Verona,\n",
            "Were young t\n",
            "#########-----------------------------------------------------------------------------------------------------------------------------###### \n",
            " \n",
            "\n",
            "REFERENCE SUMMARY: \n",
            "First Citizen:\n",
            "Before we proceed any further, hear\n",
            "\n",
            "GENERATED SUMMARY: \n",
            "First Citizen:\n",
            "He loved it.\n",
            "\n",
            "First Citizen:\n",
            "Neithe\n",
            "\n",
            "ROUGE_SCORES: \n",
            "\n",
            "{'rouge-1': {'r': 0.22748815165876776, 'p': 0.18461538461538463, 'f': 0.20382165110507086}, 'rouge-2': {'r': 0.01483679525222552, 'p': 0.014204545454545454, 'f': 0.014513783101065292}, 'rouge-l': {'r': 0.1990521327014218, 'p': 0.16153846153846155, 'f': 0.17834394409870147}}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from rouge import Rouge\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences\n",
        "block_size = 256 # what is the maximum context\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ------------\n",
        "torch.manual_seed(1337)\n",
        "with open('/content/input (1).txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "def save_checkpoint(model, optimizer, iter, filepath):\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'iteration': iter\n",
        "    }\n",
        "    torch.save(checkpoint, filepath)\n",
        "    print(f\"Model checkpoint saved at iteration {iter}.\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        perplexities = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss, perplexity = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "            perplexities[k] = perplexity.item() if perplexity is not None else float('nan')\n",
        "        out[f'{split}_loss'] = losses.mean()\n",
        "        out[f'{split}_perplexity'] = perplexities.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k =self.key(x)   # (B,T,C)\n",
        "        q =self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei =q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei =wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei =F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei =self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v =self.value(x) # (B,T,C)\n",
        "        out =wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out =torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out =self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net =nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size =n_embd // n_head\n",
        "        self.sa =MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd =FeedFoward(n_embd)\n",
        "        self.ln1 =nn.LayerNorm(n_embd)\n",
        "        self.ln2 =nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x +self.sa(self.ln1(x))\n",
        "        x = x +self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb =self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb =self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x =tok_emb + pos_emb # (B,T,C)\n",
        "        x =self.blocks(x) # (B,T,C)\n",
        "        x =self.ln_f(x) # (B,T,C)\n",
        "        logits =self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss=None\n",
        "            perplexity=None\n",
        "        else:\n",
        "            B,T,C =logits.shape\n",
        "            logits=logits.view(B*T, C)\n",
        "            targets=targets.view(B*T)\n",
        "            loss=F.cross_entropy(logits, targets)\n",
        "            probabilities=F.softmax(logits, dim=-1)\n",
        "            log_probabilities=torch.log(probabilities)\n",
        "            log_prob=torch.gather(log_probabilities,1,targets.unsqueeze(1)).squeeze(1)\n",
        "            N=targets.size(0)\n",
        "            perplexity=torch.exp(-torch.sum(log_prob)/N)\n",
        "\n",
        "        return logits, loss,perplexity\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond =idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss,perplexity = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits =logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs =F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next =torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx =torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "\n",
        "optimizer =torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train_loss']:.4f}, val loss {losses['val_loss']:.4f}, PERPLEX_TRAIN {losses['train_perplexity']:.4f} \"+\n",
        "              f\"PERPLEX_VAL {losses['val_perplexity']:.4f}\"\n",
        "        )\n",
        "        save_checkpoint(model, optimizer, iter, f\"SUPER_BIGRAM_ITER_{iter}.pt\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb =get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits,loss,perplexity = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context =torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n",
        "print(\"#########-----------------------------------------------------------------------------------------------------------------------------###### \\n \\n\")\n",
        "rouge = Rouge()\n",
        "generated_summaries = []\n",
        "context = torch.tensor([[18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10]], dtype=torch.long, device=device)\n",
        "summary= decode(m.generate(context, max_new_tokens=2000)[0].tolist())\n",
        "generated_summaries.append(summary)\n",
        "reference_summaries = []\n",
        "reference_summaries.append(text[:2000])\n",
        "print(\"REFERENCE SUMMARY: \\n\"+str(reference_summari es[0][:50])+\"\\n\")\n",
        "print(\"GENERATED SUMMARY: \\n\"+str(generated_summaries[0][:50])+\"\\n\")\n",
        "print(\"ROUGE_SCORES: \\n\")\n",
        "rouge_scores = rouge.get_scores(generated_summaries, reference_summaries)\n",
        "for score in rouge_scores:\n",
        "    print(score)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[:14]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcqIRDXznWWJ",
        "outputId": "55803b7b-53b1-48cf-923f-7ae1bd462ba0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}