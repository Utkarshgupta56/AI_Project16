# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Yno3qOIeg5TMBBGwshbG59kpGzKnAdmy
"""

# read it in to inspect it
with open('/content/drive/MyDrive/alllines.txt', 'r', encoding='utf-8') as f:
    text = f.read()

print("length of the dataset" , len(text))

print(text[:1000])

# here are all the unique characters that occur in this text
unique_chars = ''.join(sorted({char for char in text}))
vocab_size = len(unique_chars)
print(unique_chars)
print(vocab_size)

chars = sorted(set(text))
stoi = {char: i for i, char in enumerate(chars)}
itos = {i: char for i, char in enumerate(chars)}

def encode(s):
    """Encoder function: Convert a string to a list of integers based on stoi mapping."""
    return [stoi[char] for char in s]

def decode(l):
    """Decoder function: Convert a list of integers back to a string based on itos mapping."""
    return ''.join(itos[index] for index in l)
encoded_text = encode("hii there")
decoded_text = decode(encoded_text)

print(encoded_text)
print(decoded_text)

import torch

def encode_text_to_tensor(text, encode_func):
    """
    Encodes a given text using a provided encoding function and converts it into a PyTorch tensor.

    Parameters:
    - text (str): The text to be encoded.
    - encode_func (function): The function used to encode the text.

    Returns:
    - torch.Tensor: The encoded text as a PyTorch tensor.
    """
    encoded_text = encode_func(text)
    tensor = torch.tensor(encoded_text, dtype=torch.long)  #Making this into tensor
    return tensor

data = encode_text_to_tensor(text, encode)


print(data.shape, data.dtype)
print(data[:1000])

# Let's now split up the data into train and validation sets
n = int(0.85*len(data))
train_data = data[:n]
val_data = data[n:]

block_size = 8
train_data[:block_size+1]

import torch
block_size = 8

def generate_context_target_pairs(x, block_size):
    """Generate and print context-target pairs from a given sequence."""
    for t in range(block_size):
        context = x[:t+1]
        target = x[t+1] if t + 1 < len(x) else None
        print(f"when input is {context} the target: {target}")

x = train_data[:block_size]
generate_context_target_pairs(x, block_size)

# We are not feeding all the data directly into the model so but making a batch and feeding it batwise for memory management

import torch

torch.manual_seed(1337)
batch_size = 4



def get_batch(data, batch_size, block_size):
    """Generate a batch of data for inputs x and targets y."""
    ix = torch.randint(0, len(data) - block_size, (batch_size,))
    x = torch.stack([data[i:i+block_size] for i in ix])
    y = torch.stack([data[i+1:i+block_size+1] for i in ix])
    return x, y

def print_batch_details(xb, yb):
    """Print details of the input and target batches."""
    print('inputs:')
    print(xb.shape)
    print(xb)
    print('targets:')
    print(yb.shape)
    print(yb)
    print('----')

    for b in range(batch_size):
        for t in range(block_size):
            context = xb[b, :t+1].tolist()
            target = yb[b, t].item()
            print(f"when input is {context} the target: {target}")




xb, yb = get_batch(train_data, batch_size, block_size)
print_batch_details(xb, yb)

import torch
import torch.nn as nn
import torch.nn.functional as F

torch.manual_seed(1337)

class BigramLanguageModel(nn.Module):

    def __init__(self, vocab_size, embedding_dim=50):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.decoder = nn.Linear(embedding_dim, vocab_size)

    def forward(self, idx, targets=None):
        embeddings = self.embedding(idx)  # Converting indices to embedding
        logits = self.decoder(embeddings)  # assinging embeddings to vocabulary size

        loss = None
        if targets is not None:
            logits_flat = logits.view(-1, logits.size(-1))
            targets_flat = targets.view(-1)
            loss = F.cross_entropy(logits_flat, targets_flat)

        return logits, loss

    def generate(self, idx, max_new_tokens):
        with torch.no_grad():
            for _ in range(max_new_tokens):
                logits, _ = self(idx)
                probs = F.softmax(logits[:, -1, :], dim=-1)
                idx_next = torch.multinomial(probs, num_samples=1)
                idx = torch.cat([idx, idx_next], dim=1)
        return idx



m = BigramLanguageModel(vocab_size, embedding_dim=100)
logits, loss = m(xb, yb)
print(logits.shape, loss.item())

# Generate text
generated_idx = m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=10000)
generated_text = decode(generated_idx[0].tolist())
print(generated_text)