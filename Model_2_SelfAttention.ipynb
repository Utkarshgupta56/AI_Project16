{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLXHzkVRg7Tp",
        "outputId": "d48720f0-710e-40b0-8f61-bd9b328229c4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C=4,8,32\n",
        "x=torch.randn(B,T,C)\n",
        "head_size=16\n",
        "key=nn.Linear(C,head_size,bias=False)\n",
        "query=nn.Linear(C,head_size,bias=False)\n",
        "k=key(x)\n",
        "q=query(x)\n",
        "value=nn.Linear(C,head_size,bias=False)\n",
        "wei=(q@k.transpose(-2,-1))\n",
        "tril=torch.tril(torch.ones(T,T))\n",
        "wei=wei.masked_fill(tril==0,float('-inf'))\n",
        "wei=F.softmax(wei,dim=-1)\n",
        "v=value(x)\n",
        "output=wei@v\n",
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNQlTohRezA7",
        "outputId": "5997cbf3-4a1d-48c1-dba9-8e63039391fb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  def _init_(self,head_size):\n",
        "    super().__init__()\n",
        "    self.key=nn.Linear(n_embedding_vector,head_size,bias=False) ##Corresponds to the embedding layer, (see BTC shape here C is the embedding vector)\n",
        "    self.query=nn.Linear(n_embedding_vector,head_size,bias=False)\n",
        "    self.value=nn.Linear(n_embedding_vector,head_size,bias=False)\n",
        "    #Here tril is not the part of nn module to we add tril as buffer, meaning our custom function\n",
        "    self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size))) ##we made an upper triangle matrix above of shape T,T (8,8)\n",
        "\n",
        "  def forward(self,x):\n",
        "    B,T,C=x.shape\n",
        "    k=self.key(x)\n",
        "    q=self.query(x)\n",
        "\n",
        "    weight=q @ k.transpose(-2,-1) * C**-0.5\n",
        "    weight=weight.masked_fill(self.tril[:T,:T]==0,float('-inf'))\n",
        "    weight=F.softmax(weight,dim=-1)\n",
        "    v=self.value(x)\n",
        "    output=weight@v\n",
        "    return output\n",
        "\n",
        "\n",
        "'''We need to find multiple relation between the words herer , so we use multi-head-attention.\n",
        " We give 4 attention as our output to capture different relation between the words and each attention giving out 8 block of words and concenate\n",
        "them to give an output of 32 which will be equal to our n_embeddings '''\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def _init_(self,num_head,head_size):\n",
        "    super()._init_()\n",
        "    self.head=nn.ModuleList([Head(head_size) for _ in range(num_head)])\n",
        "    self.proj=nn.Linear(n_embedding_vector,n_embedding_vector)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    output=torch.cat([h(x) for h in self.heads],dim=-1) # AS THE CONCAT TO THE CHANNEL SIDE SO DIM=-1(C)\n",
        "    output=self.dropout(self.proj(output))\n",
        "    return output"
      ],
      "metadata": {
        "id": "GWYtgNQIgk7p"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from rouge import Rouge\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequence\n",
        "block_size = 8 # what is the maximum context length\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "n_head = 4\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "with open('/content/input (1).txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        perplexities = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss, perplexity = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "            perplexities[k] = perplexity.item() if perplexity is not None else float('nan')\n",
        "        out[f'{split}_loss'] = losses.mean()\n",
        "        out[f'{split}_perplexity'] = perplexities.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self,head_size):\n",
        "      super().__init__()\n",
        "      self.key=nn.Linear(n_embd,head_size,bias=False) ##Corresponds to the embedding layer, (see BTC shape here C is the embedding vector)\n",
        "      self.query=nn.Linear(n_embd,head_size,bias=False)\n",
        "      self.value=nn.Linear(n_embd,head_size,bias=False)\n",
        "      #Here tril is not the part of nn module to we add tril as buffer, meaning our custom function\n",
        "      self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size))) ##we made an upper triangle matrix above of shape T,T (8,8)\n",
        "\n",
        "#Forward process for the computation here\n",
        "    def forward(self,x):\n",
        "      B,T,C=x.shape\n",
        "      k=self.key(x)\n",
        "      q=self.query(x)\n",
        "\n",
        "      weight=q @ k.transpose(-2,-1) * C**-0.5\n",
        "      weight=weight.masked_fill(self.tril[:T,:T]==0,float('-inf'))\n",
        "      weight=F.softmax(weight,dim=-1)\n",
        "      v=self.value(x)\n",
        "      output=weight@v\n",
        "      return output\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.con_head=Head(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb =self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb =self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x =tok_emb +pos_emb # (B,T,C)\n",
        "        x =self.con_head(x) # Implementing the self_attention\n",
        "        logits =self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss=None\n",
        "            perplexity=None\n",
        "        else:\n",
        "            B,T,C =logits.shape\n",
        "            logits=logits.view(B*T, C)\n",
        "            targets=targets.view(B*T)\n",
        "            loss=F.cross_entropy(logits, targets)\n",
        "            probabilities=F.softmax(logits, dim=-1)\n",
        "            log_probabilities=torch.log(probabilities)\n",
        "            log_prob=torch.gather(log_probabilities,1,targets.unsqueeze(1)).squeeze(1)\n",
        "            N=targets.size(0)\n",
        "            perplexity=torch.exp(-torch.sum(log_prob)/N)\n",
        "\n",
        "        return logits, loss,perplexity\n",
        "\n",
        "\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond =idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits,loss,perplexity = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits =logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs =F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next =torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx =torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train_loss']:.4f}, val loss {losses['val_loss']:.4f}, PERPLEX_TRAIN {losses['train_perplexity']:.4f} \"+\n",
        "              f\"PERPLEX_VAL {losses['val_perplexity']:.4f}\"\n",
        "        )\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss,perplexity = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n",
        "\n",
        "print(\"#########-----------------------------------------------------------------------------------------------------------------------------###### \\n \\n\")\n",
        "rouge = Rouge()\n",
        "generated_summaries = []\n",
        "context = torch.tensor([[4,24,26]], dtype=torch.long, device=device)\n",
        "summary= decode(m.generate(context, max_new_tokens=2000)[0].tolist())\n",
        "generated_summaries.append(summary)\n",
        "reference_summaries = []\n",
        "reference_summaries.append(text[:2000])\n",
        "print(\"REFERENCE SUMMARY: \\n\"+str(reference_summaries[0][:50])+\"\\n\")\n",
        "print(\"GENERATED SUMMARY: \\n\"+str(generated_summaries[0][:50])+\"\\n\")\n",
        "print(\"ROUGE_SCORES: \\n\")\n",
        "rouge_scores = rouge.get_scores(generated_summaries, reference_summaries)\n",
        "for score in rouge_scores:\n",
        "    print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQX8Olz_g19n",
        "outputId": "0f6eba8f-651b-4151-90ea-5c414410344f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.007553 M parameters\n",
            "step 0: train loss 4.2000, val loss 4.2047, PERPLEX_TRAIN 66.6975 PERPLEX_VAL 67.0119\n",
            "step 500: train loss 2.6911, val loss 2.7087, PERPLEX_TRAIN 14.8039 PERPLEX_VAL 15.0633\n",
            "step 1000: train loss 2.5196, val loss 2.5303, PERPLEX_TRAIN 12.4727 PERPLEX_VAL 12.6010\n",
            "step 1500: train loss 2.4775, val loss 2.4829, PERPLEX_TRAIN 11.9448 PERPLEX_VAL 12.0245\n",
            "step 2000: train loss 2.4408, val loss 2.4523, PERPLEX_TRAIN 11.5213 PERPLEX_VAL 11.6571\n",
            "step 2500: train loss 2.4272, val loss 2.4435, PERPLEX_TRAIN 11.3702 PERPLEX_VAL 11.5503\n",
            "step 3000: train loss 2.4130, val loss 2.4327, PERPLEX_TRAIN 11.2111 PERPLEX_VAL 11.4318\n",
            "step 3500: train loss 2.3956, val loss 2.4212, PERPLEX_TRAIN 11.0012 PERPLEX_VAL 11.3028\n",
            "step 4000: train loss 2.4041, val loss 2.3992, PERPLEX_TRAIN 11.1099 PERPLEX_VAL 11.0504\n",
            "step 4500: train loss 2.3980, val loss 2.4084, PERPLEX_TRAIN 11.0406 PERPLEX_VAL 11.1476\n",
            "step 4999: train loss 2.3951, val loss 2.4126, PERPLEX_TRAIN 11.0035 PERPLEX_VAL 11.2072\n",
            "\n",
            "Shiserrs eler fo itha thes I ha thorn hif wavoufa,\n",
            "'ETCHANG ies, indd rieen eedrouh o, ld whe for day.\n",
            "ORIAMourt fo hey th, hy ntoul om see.\n",
            "\n",
            "f ane he ce avof the\n",
            "YAeretoingo. Lit out?-\n",
            "PINENUTOCAIFIcoulthe, ger bleancoover ser'd thines sb, shalilais sety I tansu cul torat t-\n",
            "I'splae pioucchere, I as, out irged wron.\n",
            "\n",
            "Wher,\n",
            "NUVMI cirut lid bleant fad I the:\n",
            "Andsifokerad theckard then.\n",
            "'T mepers?\n",
            "Whte\n",
            "Mon. I wl yovean shoulld?\n",
            "\n",
            "Bure hevend;\n",
            "Srienanged ds If!\n",
            "Asut I ar seon: wo inout bllant\n",
            "re hane fo fod ld tho, yor,\n",
            "Feald tith of quareay\n",
            "Serve quwee: he gcu frongooug shesed umeed.\n",
            "\n",
            "\n",
            "I'TALIs:\n",
            "CUCOCoe ghont bes sate:\n",
            "Th; hiris,\n",
            "I vo pee---ff idust o'sothoo,\n",
            "Thitherod ss ipre; sho noupripear dullesipronee se?\n",
            "NENGBu I ng:\n",
            "Waupt thigoretece, do;\n",
            "Angthel byomen, I woilent:\n",
            "WKE:\n",
            "Theendame,\n",
            "G mecars by hapbarend baby thad an\n",
            "Ps hiovis;\n",
            "Slh herrensee he poin ce ky I wlito you-se\n",
            "Toem'ss ait wit thean thu hy gie my ad, totimerto od? ou ircro ckn mouth hithind meam, beam hand ad'lscran fre;\n",
            "Qooulseche cchu yopenithile eenjoan llcalit, then, hithe, chy.\n",
            "\n",
            "A! le inl-d\n",
            "Thri, que bavoule,\n",
            "Wofoovendoth than ve, bar\n",
            "ts aly pl,\n",
            "D ot woul me ith peerdecameaknicerat foror he mjet! owne? windechiotegino fo, heallerrene ey?\n",
            "\n",
            "Thuprors;\n",
            "Da:\n",
            "beto, by by st wher;\n",
            "By, frany whim fbrt.\n",
            "\n",
            "Th wicee mmimon ant ft to bat aejoelene?\n",
            "\n",
            "\n",
            "Tare: ti:\n",
            "Wan I jellad CRETA:\n",
            "Ton alu lonocar't tochey aderango poacovonfouged yo hun, go dereird ar ug sse meee sse\n",
            "Th sed.\n",
            "\n",
            "A:\n",
            "MI my gel or bbopariak aveldie wan ve.\n",
            "\n",
            "Bus avey is Hertheor me.\n",
            "\n",
            "QAwime foulithe whe st: wigre kaneref barciak thintrne jue'saimy codresil ho, ant ald; kaus shit billot To Cllo, thsod?\n",
            "\n",
            "AUSICoret uink butelis likn otrifo, tan Eds, silvef youmee ithrea'sof of ll wisom'de,\n",
            "Hamaivered it ce, ameant oln sisu, ad t, fo theat.\n",
            "\n",
            "OLory VAivof ware alcru to: ard, omy sice od weangere we athe iges we Ife whe lous.\n",
            "\n",
            "Bok the gr whuwo.\n",
            "Yog wserlole,\n",
            "ma, iest; teite, thedatild de foro fo thapeste my. is st aelllo akemimy fltu r, ave at oun wethato tr uns \n",
            "#########-----------------------------------------------------------------------------------------------------------------------------###### \n",
            " \n",
            "\n",
            "REFERENCE SUMMARY: \n",
            "First Citizen:\n",
            "Before we proceed any further, hear\n",
            "\n",
            "GENERATED SUMMARY: \n",
            "&LNE:\n",
            "Yowa llsetoanl INo.\n",
            "\n",
            "Whaik:\n",
            "GUVNIE ithelow h\n",
            "\n",
            "ROUGE_SCORES: \n",
            "\n",
            "{'rouge-1': {'r': 0.08530805687203792, 'p': 0.05373134328358209, 'f': 0.06593406119195248}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.08056872037914692, 'p': 0.050746268656716415, 'f': 0.06227105752894883}}\n"
          ]
        }
      ]
    }
  ]
}