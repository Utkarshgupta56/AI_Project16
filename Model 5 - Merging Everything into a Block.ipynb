{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93667a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,num_head,head_size):\n",
    "      super().__init__()\n",
    "      self.head=nn.ModuleList([Head(head_size) for _ in range(num_head)])\n",
    "      #Adding a projection\n",
    "      self.proj=nn.Linear(n_embd,n_embd)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "      output=torch.cat([h(x) for h in self.head],dim=-1) # AS THE CONCAT TO THE CHANNEL SIDE SO DIM=-1(C)\n",
    "      output=self.proj(output)\n",
    "      return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "  def __init__(self, n_embd):\n",
    "    super().__init__()\n",
    "    self.net=nn.Sequential(\n",
    "        nn.Linear(n_embd,4* n_embd),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4* n_embd,n_embd)\n",
    "    )\n",
    "\n",
    "  def forward(self,x):\n",
    "    return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "  def __init__(self,n_embd,n_heads):\n",
    "    super().__init__()\n",
    "    self.con_head=MultiHeadAttention(n_heads,n_embd//n_heads)\n",
    "    self.ffdnn=FeedForward(n_embd)\n",
    "\n",
    "\n",
    "  def forward(self,x):\n",
    "      x=x+self.con_head(x)\n",
    "      x=x+self.ffdnn(x)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69e41b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences\n",
    "block_size = 8 # what is the maximum context length\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 32\n",
    "n_head = 4\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "with open('/content/drive/MyDrive/alllines.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self,head_size):\n",
    "      super().__init__()\n",
    "      self.key=nn.Linear(n_embd,head_size,bias=False) ##Corresponds to the embedding layer, (see BTC shape here C is the embedding vector)\n",
    "      self.query=nn.Linear(n_embd,head_size,bias=False)\n",
    "      self.value=nn.Linear(n_embd,head_size,bias=False)\n",
    "      #Here tril is not the part of nn module to we add tril as buffer, meaning our custom function\n",
    "      self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size))) ##we made an upper triangle matrix above of shape T,T (8,8)\n",
    "\n",
    "#Forward process for the computation here\n",
    "    def forward(self,x):\n",
    "      B,T,C=x.shape\n",
    "      k=self.key(x)\n",
    "      q=self.query(x)\n",
    "\n",
    "      weight=q @ k.transpose(-2,-1) * C**-0.5\n",
    "      weight=weight.masked_fill(self.tril[:T,:T]==0,float('-inf'))\n",
    "      weight=F.softmax(weight,dim=-1)\n",
    "      v=self.value(x)\n",
    "      output=weight@v\n",
    "      return output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,num_head,head_size):\n",
    "      super().__init__()\n",
    "      self.head=nn.ModuleList([Head(head_size) for _ in range(num_head)])\n",
    "      #Adding a projection\n",
    "      self.proj=nn.Linear(n_embd,n_embd)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "      output=torch.cat([h(x) for h in self.head],dim=-1) # AS THE CONCAT TO THE CHANNEL SIDE SO DIM=-1(C)\n",
    "      output=self.proj(output)\n",
    "      return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "  def __init__(self, n_embd):\n",
    "    super().__init__()\n",
    "    self.net=nn.Sequential(\n",
    "        nn.Linear(n_embd,4* n_embd),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4* n_embd,n_embd)\n",
    "    )\n",
    "\n",
    "  def forward(self,x):\n",
    "    return self.net(x)\n",
    "\n",
    "'''\n",
    "Adding the block here: with residual conections to optimzie and make GD faster.\n",
    "Looking into the paper of the residual connection. basically adding the initial feature to lmake the model\n",
    "learn the forgotten features etc.\n",
    "'''\n",
    "\n",
    "class Block(nn.Module):\n",
    "  def __init__(self,n_embd,n_heads):\n",
    "    super().__init__()\n",
    "    self.con_head=MultiHeadAttention(n_heads,n_embd//n_heads)\n",
    "    self.ffdnn=FeedForward(n_embd)\n",
    "\n",
    "\n",
    "  def forward(self,x):\n",
    "      x=x+self.con_head(x)\n",
    "      x=x+self.ffdnn(x)\n",
    "      return x\n",
    "\n",
    "\n",
    "\n",
    "# super simple bigram model-5\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks=nn.Sequential(\n",
    "            Block(n_embd,n_heads=4),\n",
    "            Block(n_embd,n_heads=4),\n",
    "            Block(n_embd,n_heads=4),\n",
    "\n",
    "        )\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb =self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb =self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x =tok_emb + pos_emb # (B,T,C)\n",
    "        x=self.blocks(x)\n",
    "        logits =self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C =logits.shape\n",
    "            logits =logits.view(B*T, C)\n",
    "            targets =targets.view(B*T)\n",
    "            loss =F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits,loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond =idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits,loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits =logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs =F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next =torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx =torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "#print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "#create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb =get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
