{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZZ2UGeomGXN",
        "outputId": "8030b181-43f3-486d-bd78-2a8e0c992738"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17.820737 M parameters\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 #\n",
        "block_size = 256 #\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 10\n",
        "n_layer = 10\n",
        "dropout = 0.2\n",
        "torch.manual_seed(1337)\n",
        "with open('/content/drive/MyDrive/input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        perplexities = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss, perplexity = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "            perplexities[k] = perplexity.item() if perplexity is not None else float('nan')\n",
        "        out[f'{split}_loss'] = losses.mean()\n",
        "        out[f'{split}_perplexity'] = perplexities.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "def save_checkpoint(model, optimizer, iter, filepath):\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'iteration': iter\n",
        "    }\n",
        "    torch.save(checkpoint, filepath)\n",
        "    print(f\"Model checkpoint saved at iteration {iter}.\")\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key=nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query=nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value=nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C=x.shape\n",
        "        k=self.key(x)   # (B,T,hs)\n",
        "        q=self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei=q@k.transpose(-2,-1) * k.shape[-1]**-0.5\n",
        "        wei=wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei=F.softmax(wei, dim=-1)\n",
        "        wei=self.dropout(wei)\n",
        "        v=self.value(x)\n",
        "        out=wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads=nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj=nn.Linear(head_size * num_heads,n_embd)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net=nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size=n_embd // n_head\n",
        "        self.sa=MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd=FeedFoward(n_embd)\n",
        "        self.ln1=nn.LayerNorm(n_embd)\n",
        "        self.ln2=nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table=nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table=nn.Embedding(block_size, n_embd)\n",
        "        self.blocks=nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f=nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head=nn.Linear(n_embd, vocab_size)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T =idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb=self.token_embedding_table(idx)\n",
        "        pos_emb=self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x=tok_emb+pos_emb\n",
        "        x=self.blocks(x)\n",
        "        x=self.ln_f(x)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss=None\n",
        "            perplexity=None\n",
        "        else:\n",
        "            B,T,C =logits.shape\n",
        "            logits=logits.view(B*T, C)\n",
        "            targets=targets.view(B*T)\n",
        "            loss=F.cross_entropy(logits, targets)\n",
        "            probabilities=F.softmax(logits, dim=-1)\n",
        "            log_probabilities=torch.log(probabilities)\n",
        "            log_prob=torch.gather(log_probabilities,1,targets.unsqueeze(1)).squeeze(1)\n",
        "            N=targets.size(0)\n",
        "            perplexity=torch.exp(-torch.sum(log_prob)/N)\n",
        "\n",
        "        return logits, loss,perplexity\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond=idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits,loss,perplexity = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits=logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs=F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next=torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx =torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "model = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
        "# checkpoint = torch.load('/content/drive/MyDrive/checkpoint_iteration_5000.pt') #map_location=device)\n",
        "checkpoint = torch.load('/content/drive/MyDrive/checkpoint_iteration_5000.pt' , map_location=torch.device('cpu'))\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer =torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.tensor([[7,58,10,6,3,26,10]],dtype=torch.long,device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nr8_t9cvsD2",
        "outputId": "466512d8-cbc9-4c5f-fafa-6b8f270b37e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-t:,$N:\n",
            "Are is thought for that?\n",
            "\n",
            "HORTENSIO:\n",
            "Go on to take; I say a pair which lives\n",
            "Then digestion by Camillo he can speak.\n",
            "\n",
            "LUCENTIO:\n",
            "A bawd, my lord! Then I bad be soft, and loves upon\n",
            "me; and every above's constant and sovereign,\n",
            "it takes a pair coaintry by John.\n",
            "\n",
            "KING RICHARD II:\n",
            "The sad indirect my behalf their heirs\n",
            "And rank what we in your hateful eyes\n",
            "Yield on or our one distress'd joy,\n",
            "And yourselves, out and the fresh soul:\n",
            "Think your heart's feed ears and what I have done\n",
            "Your highn-goodly tongue, but the other predent,\n",
            "Before I give to spend; and to your breaks\n",
            "Your appredite are distinguined\n",
            "And bitterly and your good horse with a gentleman\n",
            "Wherein your good was he subder'd.\n",
            "\n",
            "LEONTES:\n",
            "Who ill--\n",
            "\n",
            "FLORIZEL:\n",
            "Then letters defend my master\n",
            "He so from a learned preparating.\n",
            "\n",
            "POLIXENES:\n",
            "Beseech you, gentle master, fair lady,\n",
            "Tell the senate, I was at gareful, end\n",
            "'Tis general thought to give last their name;\n",
            "But Romeo prosperousion; from the royal king's,\n",
            "And mine ends a doubter slip; and so\n",
            "Now that honourlooks may it placket on fair day,\n",
            "Which these riotes, to be quiet repaired.\n",
            "\n",
            "ROMEO:\n",
            "Is it ever with a birth pride of like fathers,\n",
            "Either ten then to the earth? our brains are kind\n",
            "And yet look'd upon their perfective form?\n",
            "Would her corn away, my liege, God thus heart to quicklyHence,\n",
            "To dig to choostly to our grievous duke.\n",
            "Imphacs' mere in Vienna, and some within fire here,\n",
            "This over-must be written prophets' moist,\n",
            "Thoughts that seem'd a stoop'd for heart out\n",
            "In your king vex'd by his neck, consider,\n",
            "His fortune thirther and you both his slift,\n",
            "Aim'd in your infallight and dealing sir,\n",
            "Romeo slain and nament, telling on the fire\n",
            "Of dangerous Bolingbroke? Dares though our love\n",
            "Intender did to his liking-tent?\n",
            "Trust noble matter'd free, threefold awhile,\n",
            "And gave soldiers, and humbly allivies all\n",
            "estimes are the argument of mine,\n",
            "Virtue and three thats about my brains:\n",
            "Romeo's consent, that I can be cured\n",
            "In equital spections how it,\n",
            "And leave me against my sold\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2Wxh3Fas9YWO"
      }
    }
  ]
}